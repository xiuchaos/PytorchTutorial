{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "proposed in 2015, normalize output of each layer.\n",
    "For a batch of data, $B = \\{x_1, x_2, \\cdots, x_m\\}$\n",
    "\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
    "$$\n",
    "$$\n",
    "\\sigma^2_B = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2\n",
    "$$\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}\n",
    "$$\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T06:50:51.579067Z",
     "start_time": "2017-12-23T06:50:51.575693Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T07:14:11.077807Z",
     "start_time": "2017-12-23T07:14:11.060849Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_batch_norm_1d(x, gamma, beta):\n",
    "    eps = 1e-5\n",
    "    x_mean = torch.mean(x, dim=0, keepdim=True) # 保留维度进行 broadcast\n",
    "    x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True)\n",
    "    x_hat = (x - x_mean) / torch.sqrt(x_var + eps)\n",
    "    return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T07:14:20.610603Z",
     "start_time": "2017-12-23T07:14:20.597682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before bn: \n",
      "\n",
      "  0   1   2\n",
      "  3   4   5\n",
      "  6   7   8\n",
      "  9  10  11\n",
      " 12  13  14\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "after bn: \n",
      "\n",
      "-1.4142 -1.4142 -1.4142\n",
      "-0.7071 -0.7071 -0.7071\n",
      " 0.0000  0.0000  0.0000\n",
      " 0.7071  0.7071  0.7071\n",
      " 1.4142  1.4142  1.4142\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(15).view(5, 3)\n",
    "gamma = torch.ones(x.shape[1])\n",
    "beta = torch.zeros(x.shape[1])\n",
    "print('before bn: ')\n",
    "print(x)\n",
    "y = simple_batch_norm_1d(x, gamma, beta)\n",
    "print('after bn: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T07:32:48.025709Z",
     "start_time": "2017-12-23T07:32:48.005892Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm_1d(x, gamma, beta, is_training, moving_mean, moving_var, moving_momentum=0.1):\n",
    "    eps = 1e-5\n",
    "    x_mean = torch.mean(x, dim=0, keepdim=True) # 保留维度进行 broadcast\n",
    "    x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True)\n",
    "    if is_training:\n",
    "        x_hat = (x - x_mean) / torch.sqrt(x_var + eps)\n",
    "        moving_mean[:] = moving_momentum * moving_mean + (1. - moving_momentum) * x_mean\n",
    "        moving_var[:] = moving_momentum * moving_var + (1. - moving_momentum) * x_var\n",
    "    else:\n",
    "        x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist.MNIST('./data', train=True)\n",
    "test_set = mnist.MNIST('./data', train=False)\n",
    "\n",
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 \n",
    "    x = x.reshape((-1,)) \n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "train_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换\n",
    "test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class multi_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(multi_network, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, 100)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.layer2 = nn.Linear(100, 10)\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.randn(100))\n",
    "        self.beta = nn.Parameter(torch.randn(100))\n",
    "        \n",
    "        self.moving_mean = Variable(torch.zeros(100))\n",
    "        self.moving_var = Variable(torch.zeros(100))\n",
    "        \n",
    "    def forward(self, x, is_train=True):\n",
    "        x = self.layer1(x)\n",
    "        x = batch_norm_1d(x, self.gamma, self.beta, is_train, self.moving_mean, self.moving_var)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = multi_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 loss 函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.308139, Train Acc: 0.912797, Valid Loss: 0.181375, Valid Acc: 0.948279, Time 00:00:07\n",
      "Epoch 1. Train Loss: 0.174049, Train Acc: 0.949910, Valid Loss: 0.143940, Valid Acc: 0.958267, Time 00:00:09\n",
      "Epoch 2. Train Loss: 0.134983, Train Acc: 0.961587, Valid Loss: 0.122489, Valid Acc: 0.963904, Time 00:00:08\n",
      "Epoch 3. Train Loss: 0.111758, Train Acc: 0.968317, Valid Loss: 0.106595, Valid Acc: 0.966278, Time 00:00:09\n",
      "Epoch 4. Train Loss: 0.096425, Train Acc: 0.971915, Valid Loss: 0.108423, Valid Acc: 0.967563, Time 00:00:10\n",
      "Epoch 5. Train Loss: 0.084424, Train Acc: 0.974464, Valid Loss: 0.107135, Valid Acc: 0.969838, Time 00:00:09\n",
      "Epoch 6. Train Loss: 0.076206, Train Acc: 0.977645, Valid Loss: 0.092725, Valid Acc: 0.971420, Time 00:00:09\n",
      "Epoch 7. Train Loss: 0.069438, Train Acc: 0.979661, Valid Loss: 0.091497, Valid Acc: 0.971519, Time 00:00:09\n",
      "Epoch 8. Train Loss: 0.062908, Train Acc: 0.980810, Valid Loss: 0.088797, Valid Acc: 0.972903, Time 00:00:08\n",
      "Epoch 9. Train Loss: 0.058186, Train Acc: 0.982309, Valid Loss: 0.090830, Valid Acc: 0.972310, Time 00:00:08\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "train(net, train_data, test_data, 10, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5505\n",
      " 2.0835\n",
      " 0.0794\n",
      "-0.1991\n",
      "-0.9822\n",
      "-0.5820\n",
      " 0.6991\n",
      "-0.1292\n",
      " 2.9608\n",
      " 1.0826\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 打出 moving_mean 的前 10 项\n",
    "print(net.moving_mean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.402263, Train Acc: 0.873817, Valid Loss: 0.220468, Valid Acc: 0.932852, Time 00:00:07\n",
      "Epoch 1. Train Loss: 0.181916, Train Acc: 0.945379, Valid Loss: 0.162440, Valid Acc: 0.953817, Time 00:00:08\n",
      "Epoch 2. Train Loss: 0.136073, Train Acc: 0.958522, Valid Loss: 0.264888, Valid Acc: 0.918216, Time 00:00:08\n",
      "Epoch 3. Train Loss: 0.111658, Train Acc: 0.966551, Valid Loss: 0.149704, Valid Acc: 0.950752, Time 00:00:08\n",
      "Epoch 4. Train Loss: 0.096433, Train Acc: 0.970732, Valid Loss: 0.116364, Valid Acc: 0.963311, Time 00:00:07\n",
      "Epoch 5. Train Loss: 0.083800, Train Acc: 0.973914, Valid Loss: 0.105775, Valid Acc: 0.968058, Time 00:00:08\n",
      "Epoch 6. Train Loss: 0.074534, Train Acc: 0.977129, Valid Loss: 0.094511, Valid Acc: 0.970728, Time 00:00:08\n",
      "Epoch 7. Train Loss: 0.067365, Train Acc: 0.979311, Valid Loss: 0.130495, Valid Acc: 0.960146, Time 00:00:09\n",
      "Epoch 8. Train Loss: 0.061585, Train Acc: 0.980894, Valid Loss: 0.089632, Valid Acc: 0.974090, Time 00:00:08\n",
      "Epoch 9. Train Loss: 0.055352, Train Acc: 0.982892, Valid Loss: 0.091508, Valid Acc: 0.970431, Time 00:00:08\n"
     ]
    }
   ],
   "source": [
    "no_bn_net = nn.Sequential(\n",
    "    nn.Linear(784, 100),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(no_bn_net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1\n",
    "train(no_bn_net, train_data, test_data, 10, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 数据预处理，标准化\n",
    "    x = torch.from_numpy(x)\n",
    "    x = x.unsqueeze(0)\n",
    "    return x\n",
    "\n",
    "train_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换\n",
    "test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用批标准化\n",
    "class conv_bn_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_bn_net, self).__init__()\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 3, padding=1),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.classfy = nn.Linear(400, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classfy(x)\n",
    "        return x\n",
    "\n",
    "net = conv_bn_net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.160329, Train Acc: 0.952842, Valid Loss: 0.063328, Valid Acc: 0.978441, Time 00:00:33\n",
      "Epoch 1. Train Loss: 0.067862, Train Acc: 0.979361, Valid Loss: 0.068229, Valid Acc: 0.979430, Time 00:00:37\n",
      "Epoch 2. Train Loss: 0.051867, Train Acc: 0.984625, Valid Loss: 0.044616, Valid Acc: 0.985265, Time 00:00:37\n",
      "Epoch 3. Train Loss: 0.044797, Train Acc: 0.986141, Valid Loss: 0.042711, Valid Acc: 0.986056, Time 00:00:38\n",
      "Epoch 4. Train Loss: 0.039876, Train Acc: 0.987690, Valid Loss: 0.042499, Valid Acc: 0.985067, Time 00:00:41\n"
     ]
    }
   ],
   "source": [
    "train(net, train_data, test_data, 5, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 不使用批标准化\n",
    "class conv_no_bn_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_no_bn_net, self).__init__()\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.classfy = nn.Linear(400, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classfy(x)\n",
    "        return x\n",
    "\n",
    "net = conv_no_bn_net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.211075, Train Acc: 0.935934, Valid Loss: 0.062950, Valid Acc: 0.980123, Time 00:00:27\n",
      "Epoch 1. Train Loss: 0.066763, Train Acc: 0.978778, Valid Loss: 0.050143, Valid Acc: 0.984375, Time 00:00:29\n",
      "Epoch 2. Train Loss: 0.050870, Train Acc: 0.984292, Valid Loss: 0.039761, Valid Acc: 0.988034, Time 00:00:29\n",
      "Epoch 3. Train Loss: 0.041476, Train Acc: 0.986924, Valid Loss: 0.041925, Valid Acc: 0.986155, Time 00:00:29\n",
      "Epoch 4. Train Loss: 0.036118, Train Acc: 0.988523, Valid Loss: 0.042703, Valid Acc: 0.986452, Time 00:00:29\n"
     ]
    }
   ],
   "source": [
    "train(net, train_data, test_data, 5, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后介绍一些著名的网络结构的时候，我们会慢慢认识到批标准化的重要性，使用 pytorch 能够非常方便地添加批标准化层"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
